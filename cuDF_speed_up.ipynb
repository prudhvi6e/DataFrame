{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "_XMDuUoUl5cQ",
   "metadata": {
    "id": "_XMDuUoUl5cQ"
   },
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T1WbsSK_0Xqo",
   "metadata": {
    "id": "T1WbsSK_0Xqo"
   },
   "source": [
    "# Speed Up DataFrame Operations w/ RAPIDS cuDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-bPAvj4fwjbq",
   "metadata": {
    "id": "-bPAvj4fwjbq"
   },
   "source": [
    "## Welcome\n",
    "A **DataFrame** is a 2-dimensional data structure used to represent data in a tabular format, like a spreadsheet or SQL table. Originally offered through the Python Data Analysis ([pandas](https://pandas.pydata.org/docs/)) library, DataFrames have become very popular for its familiar representation along with a robust set of features that are intuitive and expressive. \n",
    "\n",
    "Raw data often needs to be manipulated before it can be used for further purposes such as generating **Business Intelligence**, creating **Dashboard Visualization**, or training **Machine Learning** models. These preprocessing steps can include **filtering**, **merging**, **grouping**, and **aggregating**. \n",
    "\n",
    "Below is a typical data processing pipeline: \n",
    "<p><img src='https://github.com/NVDLI/notebooks/blob/kl/cudf_speed_up/images/flow.png?raw=true' atl='flow' width=1080></p>\n",
    "\n",
    "According to [studies](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/?sh=29f71b266f63), data preparation accounts for ~80% of the work for analysts. This could be due in part to the rapid increase in the size of data as well as the iterative nature of analytics. \n",
    "\n",
    "Recognizing this potential bottleneck, NVIDIA created [**cuDF**](https://docs.rapids.ai/api/cudf/stable/) that leverages GPU hardware and software to perform data manipulation tasks with parallel computing, **saving valuable time and resources**. The cuDF library is part of the larger [**RAPIDS**](https://rapids.ai/) data science framework that allows for the execution of **end-to-end analytics pipelines** entirely on GPUs. One of the focus for cuDF and its companion suite of open source software libraries is to provide syntax that is similar to their CPU counterparts, **making it easy to implement**. \n",
    "\n",
    "This notebook is intended to demonstrate speedup in data processing by moving common DataFrame operations to the GPU with minimal changes to existing code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ComTzf6gEWwT",
   "metadata": {
    "id": "ComTzf6gEWwT"
   },
   "source": [
    "### Environment Sanity Check\n",
    "Check the output of `!nvidia-smi` to make sure you've been allocated a RAPIDS supported GPU such as Tesla T4, P4, or P100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58af14d",
   "metadata": {
    "id": "c58af14d"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GM2FQ7-P8iaF",
   "metadata": {
    "id": "GM2FQ7-P8iaF"
   },
   "source": [
    "## Interactive Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XKUJgAqC38jR",
   "metadata": {
    "id": "XKUJgAqC38jR"
   },
   "outputs": [],
   "source": [
    "import numpy as np # for generating sample data\n",
    "\n",
    "import pandas as df\n",
    "# import cudf as df\n",
    "import time # for clocking process times\n",
    "import matplotlib.pyplot as plt # for visualizing results\n",
    "\n",
    "class Timer: # creating a Timer helper class to measure execution time\n",
    "  def __enter__(self):\n",
    "    self.start=time.perf_counter()\n",
    "    return self\n",
    "  def __exit__(self, *args):\n",
    "    self.end=time.perf_counter()\n",
    "    self.interval=self.end-self.start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GjeW2Mdh0huU",
   "metadata": {
    "id": "GjeW2Mdh0huU"
   },
   "source": [
    "### Loading a Sample Data\n",
    "We start our demonstration by generating two 2-dimensional arrays of random numbers - we've configured for sizeable arrays at 1MM rows by 50 columns each. Then they are converted to DataFrames using ```pandas.DataFrame()``` or ```cudf.DataFrame()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RSCUQYModrAd",
   "metadata": {
    "id": "RSCUQYModrAd"
   },
   "outputs": [],
   "source": [
    "rows=1000000\n",
    "columns=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108eb7cb",
   "metadata": {
    "id": "108eb7cb"
   },
   "outputs": [],
   "source": [
    "def load_data(): \n",
    "  data_a=np.random.randint(0, 100, (rows, columns))\n",
    "  data_b=np.random.randint(0, 100, (rows, columns))\n",
    "  dataframe_a=df.DataFrame(data_a, columns=[f'a_{i}' for i in range(columns)])\n",
    "  dataframe_b=df.DataFrame(data_b, columns=[f'b_{i}' for i in range(columns)])\n",
    "  return dataframe_a, dataframe_b\n",
    "\n",
    "with Timer() as process_time: \n",
    "  dataframe_a, dataframe_b=load_data()\n",
    "\n",
    "print(f'The loading process took {process_time.interval:.2f} seconds')\n",
    "display(dataframe_a.tail(5))\n",
    "display(dataframe_b.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sXlraNW9cl31",
   "metadata": {
    "id": "sXlraNW9cl31"
   },
   "source": [
    "<p><img src='https://github.com/NVDLI/notebooks/blob/kl/cudf_speed_up/images/check.png?raw=true' width=720 atl='check'></p>\n",
    "\n",
    "We created two DataFrames, _dataframe_a_ and _dataframe_b_ that are 1000000 rows by 50 columns (col_1, col_2, ... col_48, col_49) each. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DKYzyh6bxwAB",
   "metadata": {
    "id": "DKYzyh6bxwAB"
   },
   "source": [
    "### Merging Data\n",
    "Sometimes data can come from multiple sources and need to be merged into one with ```DataFrame.merge()```. For example, a typical retail data storage infrastructure may include a customer table and separate transaction and product tables. Merging the data allows the correct details to be included in a single DataFrame to get the insight needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bAGSwY8qx2DB",
   "metadata": {
    "id": "bAGSwY8qx2DB"
   },
   "outputs": [],
   "source": [
    "def merge_data(left_df, right_df):\n",
    "  combined_df=df.merge(left_df, right_df, left_index=True, right_index=True)\n",
    "  return combined_df\n",
    "\n",
    "with Timer() as process_time: \n",
    "  combined_df=merge_data(dataframe_a, dataframe_b)\n",
    "\n",
    "print(f'The merging process took {process_time.interval:.2f} seconds')\n",
    "display(combined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S_1QcS17c3S5",
   "metadata": {
    "id": "S_1QcS17c3S5"
   },
   "source": [
    "<p><img src='https://github.com/NVDLI/notebooks/blob/kl/cudf_speed_up/images/check.png?raw=true' width=720 atl='check'></p>\n",
    "\n",
    "We merged two DataFrames, _dataframe_a_ and _dataframe_b_ on their _index_ into one larger DataFrame that is 1000000 rows by 100 columns (a_0, a_1, ..., b_48, b_49). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UhdsvT-gABvZ",
   "metadata": {
    "id": "UhdsvT-gABvZ"
   },
   "source": [
    "### Summarize\n",
    "Exploring data begins with **descriptive statistics**, which often involves finding the **central tendency** and **dispersion**. They are a quick way to summarize distributions. Measures of central tendency includes the mean, median, and mode - they are used to describe the center of a set of data values. Measures of dispersion include variance and standard deviation - they are used to describe the degree to which data is distributed around the center. We can quickly perform simple descriptive statistics with the ```DataFrame.describe()``` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a2c5b6",
   "metadata": {
    "id": "26a2c5b6"
   },
   "outputs": [],
   "source": [
    "def summarize(dataframe):\n",
    "  summary_df=dataframe.describe()\n",
    "  return summary_df\n",
    "\n",
    "with Timer() as process_time: \n",
    "  summary_df=summarize(combined_df)\n",
    "\n",
    "print(f'The summarizing process took {process_time.interval:.2f} seconds')\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KPz54wMldInX",
   "metadata": {
    "id": "KPz54wMldInX"
   },
   "source": [
    "<p><img src='https://github.com/NVDLI/notebooks/blob/kl/cudf_speed_up/images/check.png?raw=true' width=720 atl='check'></p>\n",
    "\n",
    "Since this is a sample data set, we see that each of columns/features (a_0, a_1, ..., b_48, b_49) have 1000000 values with an average ~50 and standard deviation of ~30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w7N64bdRAclS",
   "metadata": {
    "id": "w7N64bdRAclS"
   },
   "source": [
    "### Correlation - Exploring Relationships\n",
    "We might be interested in finding relationships/dependencies between two or more variables through their correlation with ```DataFrame.corr()```. Correlation is a number between -1 and 1 that describes the strength of the association between two variables. Two variables with a correlation of 1 suggests that they change together in the same direction while a correlation of -1 suggests that they change together in the opposite direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2538ccdd",
   "metadata": {
    "id": "2538ccdd"
   },
   "outputs": [],
   "source": [
    "def correlation(dataframe): \n",
    "  corr_df=dataframe.corr()\n",
    "  return corr_df\n",
    "\n",
    "with Timer() as process_time: \n",
    "  corr_df=correlation(combined_df)\n",
    "\n",
    "print(f'The correlation process took {process_time.interval:.2f} seconds')\n",
    "display(corr_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uaiK9t2CdgFS",
   "metadata": {
    "id": "uaiK9t2CdgFS"
   },
   "source": [
    "<p><img src='https://github.com/NVDLI/notebooks/blob/kl/cudf_speed_up/images/check.png?raw=true' width=720 atl='check'></p>\n",
    "\n",
    "The resulting cross tabulation shows that each column/feature (a_0, a_1, ..., b_48, b_49) have a perfect correlation (1) with itself and is not correlated (~0) with each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1j1Y3Y_kBYyY",
   "metadata": {
    "id": "1j1Y3Y_kBYyY"
   },
   "source": [
    "### Grouping\n",
    "We can compare subsets of the data to explore the significance of categories and classes with the ```DataFrame.groupby()``` method. We can even group continuous data values into a smaller number of bins with ```pandas.cut()``` or ```cudf.cut()``` to simplify our analysis. The groupings usually follow an aggregation such as mean or count. For example, we can group our data into 5 equidistant bins based on their sequential index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d050021a",
   "metadata": {
    "id": "d050021a"
   },
   "outputs": [],
   "source": [
    "def groupby_summarize(dataframe):\n",
    "    dataframe['group']=dataframe.index\n",
    "    dataframe['group']=df.cut(dataframe['group'], 5)\n",
    "    group_describe_df=dataframe.groupby('group').mean().reset_index(drop=True)\n",
    "    return group_describe_df\n",
    "\n",
    "with Timer() as process_time: \n",
    "    group_describe_df=groupby_summarize(combined_df)\n",
    "\n",
    "print(f'The grouping process took {process_time.interval:.2f} seconds')\n",
    "display(group_describe_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LdVGBVr9e_o8",
   "metadata": {
    "id": "LdVGBVr9e_o8"
   },
   "source": [
    "<p><img src='https://github.com/NVDLI/notebooks/blob/kl/cudf_speed_up/images/check.png?raw=true' width=720 atl='check'></p>\n",
    "\n",
    "The resulting DataFrame shows that each group maintains an average of ~50 for each column/feature (a_0, a_1, ..., b_48, b_49) as expected for this sample data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b-9gbIriKa85",
   "metadata": {
    "id": "b-9gbIriKa85"
   },
   "source": [
    "### Putting it together\n",
    "We can measure the total elapsed time for this sample data processing workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HMLKNN_RPB0c",
   "metadata": {
    "id": "HMLKNN_RPB0c"
   },
   "outputs": [],
   "source": [
    "def pipeline():\n",
    "    performance={}\n",
    "    with Timer() as process_time: \n",
    "        dataframe_a, dataframe_b=load_data()\n",
    "    performance['load data']=process_time.interval\n",
    "    with Timer() as process_time: \n",
    "        combined_df=merge_data(dataframe_a, dataframe_b)\n",
    "    performance['merge data']=process_time.interval\n",
    "    with Timer() as process_time: \n",
    "        summarize(combined_df)\n",
    "    performance['summarize']=process_time.interval\n",
    "    with Timer() as process_time: \n",
    "        correlation(combined_df)\n",
    "    performance['correlation']=process_time.interval\n",
    "    with Timer() as process_time: \n",
    "        groupby_summarize(combined_df)\n",
    "    performance['groupby & summarize']=process_time.interval\n",
    "    if df.__name__=='cudf': \n",
    "        df.DataFrame([performance], index=['gpu']).to_pandas().plot(kind='bar', stacked=True)\n",
    "    else: \n",
    "        df.DataFrame([performance], index=['cpu']).plot(kind='bar', stacked=True)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "csfRLkjsc2v8",
   "metadata": {
    "id": "csfRLkjsc2v8"
   },
   "source": [
    "### Timing the Pipeline on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8DcmBph9cyjm",
   "metadata": {
    "id": "8DcmBph9cyjm"
   },
   "outputs": [],
   "source": [
    "import pandas as df\n",
    "pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T2nKfQWD7V1k",
   "metadata": {
    "id": "T2nKfQWD7V1k"
   },
   "source": [
    "### Switching to GPU\n",
    "Traditionally, these tasks are frequently done (as we did) using the popular [**pandas**](https://pandas.pydata.org/) library, which only runs on a single CPU. NVIDIA's [**cuDF**](https://docs.rapids.ai/api/cudf/stable/) library was built with the users in mind - by offering nearly identical syntax to its CPU counterpart, developers only have to make few changes to their existing code to take advantage of its capabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TfDvbYbIU4b1",
   "metadata": {
    "id": "TfDvbYbIU4b1"
   },
   "outputs": [],
   "source": [
    "import cudf as df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oeYOoMVOLIbD",
   "metadata": {
    "id": "oeYOoMVOLIbD"
   },
   "source": [
    "**That's it!** cuDF uses nearly identical syntax to the familiar pandas API. **Brilliant!** It's worth noting that there are some features that are unique to each library, but conviniently there are a lot of overlaps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ocdd-JmXK5gg",
   "metadata": {
    "id": "Ocdd-JmXK5gg"
   },
   "outputs": [],
   "source": [
    "pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cgU3PNaPLZsS",
   "metadata": {
    "id": "cgU3PNaPLZsS"
   },
   "source": [
    "### Comparing Results\n",
    "In a trial run, **cuDF** completed the data processing tasks in nearly 10x faster than **pandas**. The expectations is that the speedup will be even more significant as the size of the data becomes largers. Feel free to give it a try by modifying the dimensions of the data above. \n",
    "\n",
    "![result](https://github.com/NVDLI/notebooks/blob/kl/cudf_speed_up/images/result.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lYPyye6BYNbr",
   "metadata": {
    "id": "lYPyye6BYNbr"
   },
   "source": [
    "## Conclusion\n",
    "Congratulations on completing the notebook! Want to learn more about cuDF and the rest of the RAPIDS framework? Check out the follow-up to this course, [Accelerating End-to-End Data Science Workflows]('https://courses.nvidia.com/courses/course-v1:DLI+S-DS-01+V1/about') or our other online courses at [NVIDIA DLI]('https://www.nvidia.com/en-us/training/online/')."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "cuDF_speed_up.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
